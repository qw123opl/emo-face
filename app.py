from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
import os
from dotenv import load_dotenv
from openai import OpenAI
import httpx # Added
import json
import re

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

app = Flask(__name__)
CORS(app)

# é…ç½®
app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY', 'dev-secret-key')
LLM_API_KEY = os.environ.get('LLM_API_KEY')
LLM_PROVIDER = os.environ.get('LLM_PROVIDER', 'openai')

# æ¨¡å‹é…ç½®
OPENAI_MODEL = os.environ.get('OPENAI_MODEL', 'gpt-4o-mini')
GEMINI_MODEL = os.environ.get('GEMINI_MODEL', 'gemini-pro')

# æ ¹æ“š provider é¸æ“‡å°æ‡‰çš„æ¨¡å‹
if LLM_PROVIDER == 'openai':
    CURRENT_MODEL = OPENAI_MODEL
elif LLM_PROVIDER == 'gemini':
    CURRENT_MODEL = GEMINI_MODEL
else:
    CURRENT_MODEL = 'gpt-4o-mini'  # é è¨­æ¨¡å‹

# åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
client = None
if LLM_PROVIDER == 'openai' and LLM_API_KEY:
    try:
        proxy_url = os.environ.get('PROXY_URL') or \
                    os.environ.get('HTTPS_PROXY') or \
                    os.environ.get('HTTP_PROXY')

        if proxy_url:
            print(f"ğŸ’¡ ä½¿ç”¨ä»£ç†ä¼ºæœå™¨ (httpx.Client): {proxy_url}")
            # Using httpx.Client directly with 'proxies' argument
            custom_httpx_client = httpx.Client(proxies=proxy_url)
            client = OpenAI(api_key=LLM_API_KEY, http_client=custom_httpx_client)
        else:
            client = OpenAI(api_key=LLM_API_KEY)

        print(f"âœ… OpenAI å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {CURRENT_MODEL}")
    except Exception as e:
        print(f"âŒ OpenAI å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
        client = None
else:
    print(f"âš ï¸ OpenAI å®¢æˆ¶ç«¯æœªåˆå§‹åŒ– - Provider: {LLM_PROVIDER}, API Key: {'å·²è¨­ç½®' if LLM_API_KEY else 'æœªè¨­ç½®'}")

def analyze_emotion(text):
    """ä½¿ç”¨ OpenAI åˆ†ææ–‡æœ¬æƒ…ç·’"""
    if not client:
        return 'neutral'
    
    try:
        response = client.chat.completions.create(
            model=CURRENT_MODEL,
            messages=[
                {
                    "role": "system", 
                    "content": "ä½ æ˜¯ä¸€å€‹æƒ…ç·’åˆ†æå°ˆå®¶ã€‚è«‹åˆ†æçµ¦å®šæ–‡æœ¬çš„æƒ…ç·’ï¼Œåªå›ç­”ä»¥ä¸‹æƒ…ç·’ä¹‹ä¸€ï¼šhappy, sad, angry, surprised, confused, excited, neutral, thoughtfulã€‚åªå›ç­”ä¸€å€‹å–®è©ï¼Œä¸è¦ä»»ä½•è§£é‡‹ã€‚"
                },
                {
                    "role": "user", 
                    "content": f"è«‹åˆ†æé€™æ®µæ–‡æœ¬çš„æƒ…ç·’ï¼š{text}"
                }
            ],
            max_tokens=10,
            temperature=0.3
        )
        
        emotion = response.choices[0].message.content.strip().lower()
        
        # é©—è­‰å›æ‡‰æ˜¯å¦åœ¨å…è¨±çš„æƒ…ç·’åˆ—è¡¨ä¸­
        valid_emotions = ['happy', 'sad', 'angry', 'surprised', 'confused', 'excited', 'neutral', 'thoughtful']
        if emotion in valid_emotions:
            return emotion
        else:
            return 'neutral'
            
    except Exception as e:
        print(f"æƒ…ç·’åˆ†æéŒ¯èª¤: {e}")
        return 'neutral'

def chat_with_llm(message):
    """èˆ‡ LLM é€²è¡Œå°è©±"""
    if not client:
        return "æŠ±æ­‰ï¼ŒLLM æœå‹™ç›®å‰ä¸å¯ç”¨ã€‚è«‹æª¢æŸ¥ API è¨­ç½®ã€‚"
    
    try:
        response = client.chat.completions.create(
            model=CURRENT_MODEL,
            messages=[
                {
                    "role": "system", 
                    "content": "ä½ æ˜¯ä¸€å€‹å‹å–„çš„ AI åŠ©æ‰‹ï¼Œåå« Emo-Faceã€‚ä½ æœƒæ ¹æ“šå°è©±å…§å®¹è¡¨é”ä¸åŒçš„æƒ…ç·’ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¿æŒå‹å–„å’Œæœ‰è¶£çš„èªèª¿ã€‚"
                },
                {
                    "role": "user", 
                    "content": message
                }
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"LLM å°è©±éŒ¯èª¤: {e}")
        return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€è¡“å•é¡Œã€‚éŒ¯èª¤ï¼š{str(e)}"

@app.route('/')
def index():
    """ä¸»é é¢"""
    return render_template('index.html')

@app.route('/health')
def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    api_status = "configured" if LLM_API_KEY else "not_configured"
    return jsonify({
        'status': 'healthy', 
        'service': 'emo-face',
        'llm_provider': LLM_PROVIDER,
        'api_status': api_status
    })

@app.route('/api/chat', methods=['POST'])
def chat():
    """èŠå¤© API ç«¯é»"""
    try:
        data = request.get_json()
        user_message = data.get('message', '')
        
        if not user_message:
            return jsonify({'error': 'æ¶ˆæ¯ä¸èƒ½ç‚ºç©º'}), 400
        
        if not LLM_API_KEY:
            return jsonify({'error': 'LLM API Key æœªé…ç½®'}), 500
        
        # ä½¿ç”¨ LLM ç”Ÿæˆå›æ‡‰
        ai_response = chat_with_llm(user_message)
        
        # åˆ†æ AI å›æ‡‰çš„æƒ…ç·’
        emotion = analyze_emotion(ai_response)
        
        response = {
            'message': ai_response,
            'emotion': emotion,
            'timestamp': '2025-06-08T12:00:00Z',
            'model': CURRENT_MODEL,
            'provider': LLM_PROVIDER
        }
        
        return jsonify(response)
    
    except Exception as e:
        print(f"èŠå¤© API éŒ¯èª¤: {e}")
        return jsonify({'error': f'æœå‹™å™¨éŒ¯èª¤: {str(e)}'}), 500

@app.route('/api/emotions')
def get_emotions():
    """ç²å–å¯ç”¨çš„æƒ…ç·’åˆ—è¡¨"""
    emotions = [
        'happy', 'sad', 'angry', 'surprised', 
        'confused', 'excited', 'neutral', 'thoughtful'
    ]
    return jsonify({'emotions': emotions})

@app.route('/api/info')
def get_api_info():
    """ç²å– API é…ç½®ä¿¡æ¯"""
    return jsonify({
        'llm_provider': LLM_PROVIDER,
        'model': CURRENT_MODEL,
        'openai_model': OPENAI_MODEL,
        'gemini_model': GEMINI_MODEL,
        'api_configured': bool(LLM_API_KEY),
        'supported_emotions': [
            'happy', 'sad', 'angry', 'surprised', 
            'confused', 'excited', 'neutral', 'thoughtful'
        ]
    })

@app.route('/api/test', methods=['GET'])
def test_api():
    """æ¸¬è©¦ API é€£æ¥æ˜¯å¦æ­£å¸¸"""
    if not LLM_API_KEY:
        return jsonify({
            'status': 'error',
            'message': 'API Key æœªé…ç½®',
            'api_configured': False
        }), 400
    
    if not client:
        return jsonify({
            'status': 'error',
            'message': 'OpenAI å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–',
            'api_configured': False
        }), 500
    
    try:
        # ç™¼é€ä¸€å€‹ç°¡å–®çš„æ¸¬è©¦è«‹æ±‚
        test_response = client.chat.completions.create(
            model=CURRENT_MODEL,
            messages=[
                {
                    "role": "user", 
                    "content": "è«‹å›ç­”ï¼šæ¸¬è©¦æˆåŠŸ"
                }
            ],
            max_tokens=10,
            temperature=0
        )
        
        response_text = test_response.choices[0].message.content.strip()
        
        return jsonify({
            'status': 'success',
            'message': 'API é€£æ¥æ­£å¸¸',
            'test_response': response_text,
            'model': CURRENT_MODEL,
            'provider': LLM_PROVIDER,
            'api_configured': True
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'API èª¿ç”¨å¤±æ•—: {str(e)}',
            'api_configured': True,
            'error_details': str(e)
        }), 500

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    debug = os.environ.get('FLASK_DEBUG', 'False').lower() == 'true'
    app.run(host='0.0.0.0', port=port, debug=debug)
